{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054068bc-bf8d-4ce5-8e45-ad57bc3bf099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import siibra\n",
    "# for plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ac553-8eaa-4690-a032-cfc9f0bd6916",
   "metadata": {},
   "source": [
    "# Define region of interest map\n",
    "\n",
    "Here we define a region of interest by annotation a point in BigBrain space, and building a heatmap volume of the point with a certain uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62c2631-a495-43d9-81a8-25d90080c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = siibra.from_json(\"\"\"\n",
    "<INSERT ANNOTATION FROM SIIBRA-EXPLORER HERE>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74612f33-7488-4a95-8f24-ebd211d3239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the point to an image volume with uncertainty radius\n",
    "pc = siibra.PointCloud([pt], space='bigbrain', sigma_mm=0.3)\n",
    "query_volume = siibra.volumes.from_pointcloud(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407c69c-6211-464b-b621-15079a3b61c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot this query volume on top of a lower-resolution copy of BigBrain\n",
    "bigbrain = siibra.get_template('bigbrain').fetch()\n",
    "f = plt.figure(figsize=(13, 4))\n",
    "plotting.plot_stat_map(query_volume.fetch(), bg_img=bigbrain, cmap='viridis', figure=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949bdd44-31c7-49f4-bdd1-8d53a0baab9f",
   "metadata": {},
   "source": [
    "# Sample cortical image patches inside query region\n",
    "\n",
    "The volume can be used to let siibra sample cortical patches from BigBrain 1 micron data.\n",
    "For this purpose, siibra samples vertices on the cortical midsurface close to the region of interest, and derives oriented patch bounding boxes using the layer surface geometry. \n",
    "The number of proposed patches depends on the size of the region of interest - larger heatmaps will result in more patches and longer runtimes for the query.\n",
    "The query can take some time, since siibra resamples image data into oriented patches.\n",
    "The resulting image objects come with spatial metadata that preserves their anchoring in the BigBrain space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a54651-4e1d-4d1c-99bc-587cf5a6d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run feature query for cortical patch extraction.\n",
    "# this can take a little, since suitable sample patches will be resampled into upright position.\n",
    "features = siibra.features.get(query_volume, siibra.features.cellular.BigBrain1MicronPatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150dc553-31cf-4732-ad66-4eb1081c84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the first of the resulting patches.\n",
    "assert len(features) > 0\n",
    "patch = features[0]\n",
    "patch_img = patch.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32377db-2338-43f2-98bb-1efd5bb188eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch layer mask for the patch\n",
    "layermaps = siibra.get_map('layers', space='bigbrain')\n",
    "voi = patch.get_boundingbox()\n",
    "layermask = layermaps.fetch(fragment='left', format='image', voi=voi, resolution_mm=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c624171a-602e-41ac-8545-6405d6176850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the patch in 3D context as well as the corresponding layer mask.\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,6))\n",
    "plotting.plot_img(patch_img, bg_img=bigbrain, display_mode='tiled', axes=ax1)\n",
    "plotting.plot_img(patch_img, bg_img=None, display_mode='y', cmap='gray', axes=ax2, cut_coords=[voi.minpoint[1]])\n",
    "plotting.plot_img(layermask, bg_img=None, display_mode='y', cmap='Set1', axes=ax3, cut_coords=[voi.minpoint[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6844961-31a9-48e6-a9f9-346cb73401b1",
   "metadata": {},
   "source": [
    "# Use an AI model to detect cells in the patch\n",
    "\n",
    "We use a pre-trained Contour Proposal Network (Eric Upschulte et al.) from celldetection.org to segment cell bodies in the resulting patch image.\n",
    "For this, we reformat the patch into a 3-channel float image.\n",
    "To reduce runtime and memory needs of this notebook, we limit the extraction to a subwindow in side the patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4947459-626f-47ac-9f7a-b373bf54492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import celldetection as cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe9662-babb-4567-a5a4-c6f7885b7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve CPN model for cell detection\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = cd.fetch_model('vacumu_CpnResNeXt101UNet-f33b2634bb51f299').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566426fe-18bd-46a8-832a-f44bd5da213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format into 3-channel 2D-image for cell detection\n",
    "img2D = patch_img.get_fdata().squeeze()[2100:2900, :500] / 2**16\n",
    "img2D_3ch = np.stack((img2D,) * 3, 2)\n",
    "\n",
    "# run cell detection\n",
    "x = cd.to_tensor(img2D_3ch, transpose=True, device=device, dtype=torch.float32)\n",
    "x = x[None]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa4d27-3ac5-4cd5-b053-11508a5385e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the locations\n",
    "contours = y[\"contours\"][0].cpu().data.numpy()\n",
    "plt.figure()\n",
    "plt.imshow(img2D, cmap='gray')\n",
    "for c in contours:\n",
    "    X, Y = c.T\n",
    "    plt.plot(X, Y, '-', lw=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1444861-6fd9-4695-895f-ce04dd4cd20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
